# üöÄ Kubernetes Part 1 ‚Äì minikube & KOps Installation

This README serves as a **complete hands-on guide** for Kubernetes learning. It includes YAML manifests and command-line usage to help you practice kubernetes related topics.

## üìö What You‚Äôll Learn

- ‚úÖ How to create a Production Grade Cluster using **kOps**
- ‚úÖ How to create a Local **minikube** Cluster

---

## üîß Getting Started with kOps on AWS

[Click me](https://kops.sigs.k8s.io/getting_started/aws/) for Official Documentation of kOps

### 1Ô∏è‚É£ Install Kops for Linux

```bash
curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
chmod +x ./kops
sudo mv ./kops /usr/local/bin/
```

### 2Ô∏è‚É£ Install kubectl for Linux

```bash
curl -Lo kubectl https://dl.k8s.io/release/$(curl -s -L https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl
```

### 3Ô∏è‚É£ Setup IAM user 

In order to build clusters within AWS we'll create a dedicated IAM user for kops. This user requires API credentials in order to use kops. Create the user, and credentials, using the AWS console.

The kops user will require the following IAM permissions to function properly:

```bash
AmazonEC2FullAccess
AmazonRoute53FullAccess
AmazonS3FullAccess
IAMFullAccess
AmazonVPCFullAccess
AmazonSQSFullAccess
AmazonEventBridgeFullAccess
```

### 4Ô∏è‚É£ Create S3 Bucket 

In order to store the state of your cluster, and the representation of your cluster, we need to create a dedicated S3 bucket for kops to use. This bucket will become the source of truth for our cluster configuration. In this guide we'll call this bucket ==**lwm-kubernetes-bucket**==, but you should add a custom prefix as bucket names need to be unique.

### 5Ô∏è‚É£ Prepare local environment 

You should record the SecretAccessKey and AccessKeyID in the returned JSON output, and then use them below:

```bash
# configure the aws client to use your new IAM user
aws configure           # Use your new access and secret key here
aws iam list-users      # you should see a list of all your IAM users here

# Create ssh-keys for kOps to use
ssh-keygen -t rsa -b 4096 -f ~/.ssh/kops-ssh-key
kops create secret --name <your-cluster-name> sshpublickey admin -i ~/.ssh/kops-ssh-key.pub

# Because "aws configure" doesn't export these vars for kops to use, we export them now
export AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id)
export AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)
export NAME=myfirstcluster.k8s.local
export KOPS_STATE_STORE=s3://prefix-example-com-state-store
```

For a gossip-based cluster, make sure the name ends with ==**k8s.local.**==

### 6Ô∏è‚É£ kOps Commands

```bash
kops create cluster --name=${NAME} --cloud=aws --zones=ap-southeast-1a
kops edit cluster --name ${NAME}
kops update cluster --name ${NAME} --yes --admin
kops validate cluster
kops delete cluster --name ${NAME} --yes
```

---

## üîß Getting Started with minikube on AWS

[Click me](https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download) for Official Documentation of minikube

### 1Ô∏è‚É£ Installation

Create a Linux EC2 VM with minimum  
- 2 CPUs or more 
- 2GB of free memory 
- 20GB of free disk space

```bash
curl -LO https://github.com/kubernetes/minikube/releases/latest/download/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube && rm minikube-linux-amd64
```

### 2Ô∏è‚É£ Start your cluster

```bash
minikube start
```

### 3Ô∏è‚É£ Interact with your cluster

```bash
minikube kubectl -- get pod -A
```

You can also make your life easier by adding the following to your shell config

```bash
alias kubectl="minikube kubectl --"
kubectl get pod
```

# üöÄ Kubernetes Part 2 ‚Äì YAML Reference Guide

In Part 2, you will learn how to create **Namespaces, Pods, ReplicaSets, DaemonSets.** Learn about kubectl cheat codes and both imperative and declarative way of deploying kubernetes Objects


## üìö What You‚Äôll Learn

- ‚úÖ Creating and using **Namespaces**
- ‚úÖ Defining **Pods** with one or more containers
- ‚úÖ Using **ReplicaSets** to maintain pod availability
- ‚úÖ Deploying **DaemonSets** to run a pod on each node

---

## üîß How to Use

üëâ You can copy all YAMLs below into a file like `main.yaml` and run:

```bash
kubectl apply -f main.yaml
```

### 1Ô∏è‚É£ Namespace: qa

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: qa
```

### 2Ô∏è‚É£ Pod: myfirstpod (Single Container)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myfirstpod
spec:
  containers:
  - name: cont1
    image: httpd
    ports:
    - containerPort: 80
```

### 3Ô∏è‚É£ Pod: mysecondpod (Multi-Container: httpd + jenkins)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mysecondpod
spec:
  containers:
  - name: cont1
    image: httpd
    ports:
    - containerPort: 80
  - name: cont2
    image: jenkins/jenkins
    ports:
    - containerPort: 8080
```

### 4Ô∏è‚É£ Pod: mythirdpod (Multi-Container: httpd + nginx)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mythirdpod
spec:
  containers:
  - name: cont1
    image: httpd
    ports:
    - containerPort: 80
  - name: cont2
    image: nginx
    ports:
    - containerPort: 80
```

### 5Ô∏è‚É£ Pod: myfirstpod in dev Namespace

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myfirstpod
  namespace: dev
spec:
  containers:
  - name: cont1
    image: httpd
    ports:
    - containerPort: 80
```

### 6Ô∏è‚É£ ReplicaSet: lwm-replica (5 Replicas)

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-replica
spec:
  # modify replicas according to your case
  replicas: 5
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: cont1
        image: httpd
```

### 7Ô∏è‚É£ DaemonSet: lwm-daemon

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-daemon
spec:
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: cont1
        image: httpd
```

# üöÄ Kubernetes Part 3 ‚Äì YAML Reference Guide

In **Part 3**, you will learn how to manage applications using **Deployments** and expose them to users and other services using **Kubernetes Services**.


## üìö What You‚Äôll Learn

- ‚úÖ Understand what a Deployment is and how it manages Pods  
- ‚úÖ Create Deployments with different update strategies (RollingUpdate vs Recreate)  
- ‚úÖ Expose Pods and Deployments using various Service types:
    - üîπ **NodePort** ‚Äì for accessing Pods externally via a fixed port on the node  
    - üîπ **ClusterIP** ‚Äì for internal Pod-to-Pod communication  
    - üîπ **LoadBalancer** ‚Äì for external access using cloud provider‚Äôs load balancer
- ‚úÖ Demonstrate how **selectors and labels** work in real-world scenarios  
- ‚úÖ Connect Pods internally and externally using appropriate Service types  
- ‚úÖ Expose applications like **Jenkins** via NodePort  
- ‚úÖ Test and validate connections using port mappings

---

## üîß How to Use

üëâ You can copy all YAMLs below into a file like `main.yaml` and run:

```bash
kubectl apply -f main.yaml
```

### 1Ô∏è‚É£ Basic Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: cont1
        image: httpd
```

### 2Ô∏è‚É£ Deployment with Recreate Strategy

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment-re
spec:
  strategy:
    type: Recreate
  replicas: 4
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: cont1
        image: httpd

```

### 3Ô∏è‚É£ Exposing a Pod with NodePort Service

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webserver
  labels:
    app: httpd
spec:
  containers:
  - name: cont1
    image: httpd
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: my-np-service
spec:
  type: NodePort
  selector:
    app: httpd
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30001
```

### 4Ô∏è‚É£ Exposing Jenkins Pod with NodePort Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-np-service
spec:
  type: NodePort
  selector:
    youtube: lwm
  ports:
    - port: 80
      targetPort: 8080
      nodePort: 30002
---
apiVersion: v1
kind: Pod
metadata:
  name: jenkins-pod
  labels:
    youtube: lwm
spec:
  containers:
  - name: cont1
    image: jenkins/jenkins
    ports:
    - containerPort: 8080
```

### 5Ô∏è‚É£ Pod-to-Pod Connection via ClusterIP Service

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    colour: black
spec:
  containers:
  - name: cont1
    image: httpd
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: pod2
  labels:
    colour: pink
spec:
  containers:
  - name: cont1
    image: nginx
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: my-cip-service
spec:
  type: ClusterIP
  selector:
    colour: pink
  ports:
    - port: 9999
      targetPort: 80
```

### 6Ô∏è‚É£ Exposing Deployment with LoadBalancer Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: lb-service
spec:
  type: LoadBalancer
  selector:
    colour: blue
  ports:
    - port: 80
      targetPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      colour: blue
  template:
    metadata:
      labels:
        colour: blue
    spec:
      containers:
      - name: cont1
        image: httpd
```

### 7Ô∏è‚É£ Showcasing Selector and Labels with Deployment and Pod

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      colour: blue
  template:
    metadata:
      labels:
        colour: blue
    spec:
      containers:
      - name: cont1
        image: httpd
---
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    colour: blue
spec:
  containers:
  - name: cont1
    image: nginx
    ports:
    - containerPort: 80
```

# üöÄ Kubernetes Part 4 ‚Äì YAML Reference Guide

In **Part 4**, you will learn how to securely manage user access and permissions inside your EKS cluster using:

## üìö What You‚Äôll Learn

- ‚úÖ `aws-auth` ConfigMap to add IAM users and roles  
- ‚úÖ Role-Based Access Control (RBAC) for namespace-specific and cluster-wide access  
- ‚úÖ Create `Role`, `RoleBinding`, `ClusterRole`, and `ClusterRoleBinding`  
- ‚úÖ Map IAM users to Kubernetes RBAC groups  
- ‚úÖ Use **IRSA (IAM Roles for Service Accounts)** to allow Pods to access AWS services like S3 securely  
- ‚úÖ Hands-on live demo of a Pod accessing S3 using IRSA

---

## üîß How to Use

üëâ You can copy all YAMLs below into a file like `main.yaml` and run:

```bash
kubectl apply -f main.yaml
```

### 1Ô∏è‚É£ aws-auth ConfigMap ‚Äì Adding IAM Users

```yaml
apiVersion: v1
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::992382429239:role/eks-node-group-role
      username: system:node:{{EC2PrivateDNSName}}
  mapUsers: |
    - userarn: arn:aws:iam::992382429239:user/dev-user
      username: dev-user
      groups:
        - dev-group
    - userarn: arn:aws:iam::992382429239:user/readonly-user
      username: readonly-user
      groups:
        - readonly-group
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
```

### 2Ô∏è‚É£ Dev User Role and RoleBinding (Namespace-scoped Access)

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: dev-role
  namespace: dev-namespace
rules:
- apiGroups: ["", "apps", "batch"]
  resources: ["pods", "services", "deployments", "jobs"]
  verbs: ["get", "list", "create", "update", "delete", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-binding
  namespace: dev-namespace
subjects:
- kind: Group
  name: dev-group
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: dev-role
  apiGroup: rbac.authorization.k8s.io
```

### 3Ô∏è‚É£ ReadOnly User ClusterRole and ClusterRoleBinding
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: readonly-role
rules:
- apiGroups: ["", "apps", "batch"]
  resources: ["pods", "services", "deployments", "jobs"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: readonly-binding
subjects:
- kind: Group
  name: readonly-group
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: readonly-role
  apiGroup: rbac.authorization.k8s.io
```

### 4Ô∏è‚É£ Pod Accessing S3 Using IRSA

#### üîπ Step 1: Enable OIDC Provider for Your EKS Cluster

```bash
eksctl utils associate-iam-oidc-provider \
  --region ap-southeast-1 \
  --cluster <cluster-name> \
  --approve
```

#### üîπ Step 2: Create IAM Role with Trust Policy for Pod
#### üîπ Step 3: Create Kubernetes ServiceAccount

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: s3-access
  namespace: default
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::<account-id>:role/EKS_S3_IRSA_Role
```

#### üîπ Step 4: Deploy a Pod Using the Service Account

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: s3-reader
spec:
  serviceAccountName: s3-access
  containers:
  - name: awscli
    image: amazonlinux
    command: ["/bin/sh"]
    args: ["-c", "yum install -y aws-cli && aws s3 ls"]
```

#### üîπ Step 5: Verify Access to S3 from Pod

```bash
2025-06-01 00:00:00 lwm-terraform-bucket
2025-06-01 00:00:00 lwm-kubernetes-bucket
```

# üöÄ Kubernetes Part 5 ‚Äì YAML Reference Guide

In **Part 5**, you will learn advanced pod scheduling techniques in Kubernetes to control how and where your Pods run within your EKS cluster.

## üìö What You‚Äôll Learn

 - ‚úÖ **Node Selector** ‚Äì Assign Pods to specific nodes using simple labels
 - ‚úÖ **Node Affinity** ‚Äì Fine-grained control over Pod placement using label expressions
 - ‚úÖ **Pod Affinity & Anti-Affinity** ‚Äì Co-locate or separate Pods based on labels and topology
 - ‚úÖ **Taints and Tolerations** ‚Äì Ensure only specific Pods are scheduled on tainted nodes
 - ‚úÖ Use **requiredDuringSchedulingIgnoredDuringExecution** and understand its impact
 - ‚úÖ Real-world examples with **multiple tolerations** (e.g., green, blue)
 - ‚úÖ Hands-on live demo with YAML files showing each scheduling strategy in action

---

## üõ†Ô∏è Cluster Setup: Three Worker Nodes in EKS

We will create three EKS worker nodes with custom labels and taints.

### ‚úÖ Step-by-Step

1. Launch EKS cluster with 3 managed node groups (use console)
2. After nodes are ready, label and taint the nodes using kubectl: (in video the taint and labels are added in console you can also use the below commands)

```bash
# Label node 1 as general
kubectl label node <node-name-1> node-type=general

# Label and taint node 2 as high-cpu with taint colour=green
kubectl label node <node-name-2> node-type=high-cpu
kubectl taint node <node-name-2> colour=green:NoSchedule

# Label and taint node 3 as high-memory with taint colour=blue
kubectl label node <node-name-3> node-type=high-memory
kubectl taint node <node-name-3> colour=blue:NoSchedule
```

---

## üîß How to Use

üëâ You can copy all YAMLs below into a file like `main.yaml` and run:

```bash
kubectl apply -f main.yaml
```

### 1Ô∏è‚É£ Basic Pod on Any Node

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
```

### 2Ô∏è‚É£ Pod with Required Node Affinity (high-memory)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: memory-intensive-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-type
            operator: In
            values:
            - high-memory
  containers:
  - name: memory-app
    image: nginx
```

### 3Ô∏è‚É£ Pod with Preferred Node Affinity (high-memory)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: memory-intensive-pod
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: node-type
            operator: In
            values:
            - high-memory
  containers:
  - name: memory-app
    image: nginx
```

### 4Ô∏è‚É£ Pod with Toleration for Taint (colour=green:NoSchedule)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: cpu-intensive-pod
spec:
  tolerations:
  - key: "colour"
    operator: "Equal"
    value: "green"
    effect: "NoSchedule"
  containers:
  - name: cpu-app
    image: nginx
```

### 5Ô∏è‚É£ Deployment Tolerating Green Taint

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: green-app-deployment
  labels:
    app: green-app
spec:
  replicas: 5
  selector:
    matchLabels:
      app: green-app
  template:
    metadata:
      labels:
        app: green-app
    spec:
      tolerations:
      - key: "colour"
        operator: "Equal"
        value: "green"
        effect: "NoSchedule"
      containers:
      - name: green-container
        image: nginx
        ports:
        - containerPort: 80
```

### 6Ô∏è‚É£ Deployment Tolerating Both green and blue Taints

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multicolor-app
  labels:
    app: multicolor-app
spec:
  replicas: 8
  selector:
    matchLabels:
      app: multicolor-app
  template:
    metadata:
      labels:
        app: multicolor-app
    spec:
      tolerations:
      - key: "colour"
        operator: "Equal"
        value: "green"
        effect: "NoSchedule"
      - key: "colour"
        operator: "Equal"
        value: "blue"
        effect: "NoSchedule"
      containers:
      - name: multicolor-container
        image: nginx
        ports:
        - containerPort: 80
```

### 7Ô∏è‚É£ Deployment with Node Affinity and Toleration

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: compute-app-deployment
  labels:
    app: compute-app
spec:
  replicas: 5
  selector:
    matchLabels:
      app: compute-app
  template:
    metadata:
      labels:
        app: compute-app
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-type
                operator: In
                values:
                - high-cpu
      tolerations:
      - key: "colour"
        operator: "Equal"
        value: "green"
        effect: "NoSchedule"
      containers:
      - name: compute-container
        image: nginx
        ports:
        - containerPort: 80
```

### 8Ô∏è‚É£ Pod with Label for Pod Affinity Targeting

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    group: bestfriendz 
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
```

### 9Ô∏è‚É£ Pod Affinity (Schedule with bestfriendz Pods)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: httpd-pod
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            group: bestfriendz
        topologyKey: "kubernetes.io/hostname"
  containers:
  - name: cont1
    image: httpd
```

### 1Ô∏è‚É£0Ô∏è‚É£ Pod Anti-Affinity (Avoid bestfriendz Pods)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: enemy-pod
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            group: bestfriendz
        topologyKey: "kubernetes.io/hostname"
  containers:
  - name: cont1
    image: httpd
```

# üöÄ Kubernetes Part 6 ‚Äì YAML Reference Guide

In **part 6**, you'll explore how to persist and share data between Pods using different types of volumes. You‚Äôll also understand how to dynamically and statically provision storage in AWS with EBS and EFS.

## üìö What You‚Äôll Learn

- ‚úÖ Difference between `emptyDir`, `hostPath`, `PersistentVolume` (PV) and `PersistentVolumeClaim` (PVC)  
- ‚úÖ Use `StorageClass` for dynamic provisioning of **EBS** and **EFS**  
- ‚úÖ Understand how `ReadWriteOnce` and `ReadWriteMany` access modes affect Pod mounting  
- ‚úÖ Share data between containers using `emptyDir`  
- ‚úÖ Mount host machine files into Pods using `hostPath`  
- ‚úÖ Use **EBS** volumes with static and dynamic provisioning  
- ‚úÖ Use **EFS** for shared, scalable, and dynamic file storage  
- ‚úÖ Full hands-on examples with YAML

---

### üõ†Ô∏è Cluster Setup: Enable the Add-on with Pod Identity (via Console)

- 1Ô∏è‚É£ Go to EKS > Your Cluster > Add-ons
- 2Ô∏è‚É£ Click Create Add-on
- 3Ô∏è‚É£ Choose aws-ebs-csi-driver & aws-efs-csi-driver
- 4Ô∏è‚É£ Select latest version
- 5Ô∏è‚É£ For IAM Role, choose:
- 6Ô∏è‚É£ Pod Identity (recommended)
- 7Ô∏è‚É£ Select or create the IAM role: AmazonEKS_EBS_CSI_DriverRole, AmazonEKS_EFS_CSI_DriverRole

---

# üì¶ Kubernetes Volumes ‚Äì Quick Notes

## ‚ö†Ô∏è Pods are Ephemeral!
 ==When a Pod is deleted, everything inside it is lost ‚Äî unless it's backed by a volume.==

---

## üìÅ Volume Types

| Volume Type   | Description                                             |
|---------------|---------------------------------------------------------|
| `emptyDir`    | Temporary storage shared between containers in a Pod. Deleted when the Pod is deleted. |
| `hostPath`    | Mounts a file or directory from the host node into the Pod. **Not recommended** for production use. |
| **Persistent Volumes** | Backed by real storage from cloud or local systems. Persist beyond Pod lifecycle. |
| ‚îú‚îÄ EBS         | Amazon Elastic Block Store ‚Äî supports **RWO** access. |
| ‚îî‚îÄ EFS         | Amazon Elastic File System ‚Äî supports **RWX** access. |

---

## ‚öôÔ∏è Provisioning Modes

| Mode               | Description                                              |
|--------------------|----------------------------------------------------------|
| **Static**         | Volume is pre-created manually (e.g., via AWS Console)   |
| **Dynamic**        | Volume is created **automatically** using a `StorageClass` |

---

## üîì Access Modes

| Access Mode         | Description                                           | Example Volume |
|---------------------|-------------------------------------------------------|----------------|
| `ReadWriteOnce` (RWO) | One node can read/write at a time                   | EBS            |
| `ReadOnlyMany` (ROX)  | Many nodes can read, but none can write             | EFS (rare)     |
| `ReadWriteMany` (RWX) | Many nodes can read/write simultaneously            | EFS            |
| `ReadWriteOncePod`    | Only a single pod (on any node) can access the volume | Rare use case  |

---

## ‚ôªÔ∏è Reclaim Policies

| Policy     | Description                                                      |
|------------|------------------------------------------------------------------|
| `Retain`   | Manual cleanup required. Volume and data are retained.          |
| `Recycle`  | Performs basic scrub (`rm -rf /volume/*`) ‚Äî deprecated in many environments |
| `Delete`   | Deletes the associated storage resource automatically (e.g., EBS volume) |

---

## üîÅ volumeBindingMode

| Mode                  | Behavior                                                              |
|-----------------------|-----------------------------------------------------------------------|
| `WaitForFirstConsumer` | Volume is created **only when Pod is scheduled** (prevents cross-AZ issues) |
| `Immediate`           | Volume is created **as soon as PVC is created**, even before pod uses it |

---

## üîß How to Use

üëâ You can copy all YAMLs below into a file like `main.yaml` and run:

```bash
kubectl apply -f main.yaml
```

### 1Ô∏è‚É£ Sharing Data Between Containers Using `emptyDir`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: emptydir-example
spec:
  containers:
    - name: writer-cont1
      image: busybox
      command: ["/bin/sh", "-c", "echo 'Hello from writer' > /data/message.txt && sleep 3600"]
      volumeMounts:
        - mountPath: /data
          name: shared-data
    - name: reader-cont2
      image: busybox
      command: ["/bin/sh", "-c", "cat /data/message.txt && sleep 3600"]
      volumeMounts:
        - mountPath: /data
          name: shared-data
  volumes:
    - name: shared-data
      emptyDir: {}
```

### 2Ô∏è‚É£ Accessing Host Files Using hostPath

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-etc-example
spec:
  containers:
    - name: etc-reader
      image: busybox
      command: ["/bin/sh", "-c", "ls /host/etc && sleep 3600"]
      volumeMounts:
        - mountPath: /host/etc
          name: host-etc
  volumes:
    - name: host-etc
      hostPath:
        path: /etc
        type: Directory
```

### 3Ô∏è‚É£ Dynamic EBS Provisioning Using StorageClass and PVC

```yaml
# StorageClass for EBS
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-sc
provisioner: ebs.csi.aws.com
volumeBindingMode: WaitForFirstConsumer
---
# PVC for EBS
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 4Gi
  storageClassName: ebs-sc
---
# Pod using the dynamic EBS volume
apiVersion: v1
kind: Pod
metadata:
  name: ebs-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - mountPath: "/data"
      name: ebs-volume
  volumes:
  - name: ebs-volume
    persistentVolumeClaim:
      claimName: ebs-pvc

```

### 4Ô∏è‚É£ Static EBS Volume with Manual PV and PVC

```yaml
# Static PersistentVolume (replace with actual EBS Volume ID)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: static-ebs-pv
spec:
  capacity:
    storage: 3Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  csi:
    driver: ebs.csi.aws.com
    volumeHandle: vol-xxxxxxxxxxxxxx     # Replace EBS Volume id
---
# PersistentVolumeClaim
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: static-ebs-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: manual
  resources:
    requests:
      storage: 3Gi
---
# Pod using the static EBS volume
apiVersion: v1
kind: Pod
metadata:
  name: ebs-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - mountPath: "/data"
      name: ebs-volume
  volumes:
  - name: ebs-volume
    persistentVolumeClaim:
      claimName: static-ebs-pvc

```

### 5Ô∏è‚É£ Dynamic EFS Volume Provisioning with StorageClass and PVC

```yaml
# StorageClass for EFS
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap
  fileSystemId: fs-0f8431a1f00634456  # Replace with your EFS ID
  directoryPerms: "700"
  gidRangeStart: "1000"
  gidRangeEnd: "2000"
  basePath: "/dynamic_provisioning"
---
# PVC for EFS
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  storageClassName: efs-sc
---
# Pod using EFS volume
apiVersion: v1
kind: Pod
metadata:
  name: efs-test-pod
spec:
  containers:
    - name: app
      image: busybox
      command: [ "/bin/sh", "-c", "echo EFS volume is working > /data/hello.txt && sleep 3600" ]
      volumeMounts:
        - name: efs-volume
          mountPath: /data
  volumes:
    - name: efs-volume
      persistentVolumeClaim:
        claimName: efs-pvc
```
# üö¶ Kubernetes Part 7 ‚Äì Probes, Init Containers, Sidecars & Environment Variables


## üìö What You Will Learn

In **Day 7**, you'll learn how Kubernetes manages **application health**, **startup sequences**, and **shared responsibilities** between containers. This session is packed with **live examples** and advanced techniques.

‚úÖ Understand how **Liveness**, **Readiness**, and **Startup** probes work  
‚úÖ Configure probes to detect failures, slow startups, and readiness checks  
‚úÖ Use **Init Containers** for setup tasks before app starts  
‚úÖ Implement the **Sidecar pattern** to enhance or monitor your main container  
‚úÖ Inject configuration using **Environment Variables**, **ConfigMaps**, and **Secrets**  
‚úÖ Mount secrets as files and access securely

---

## üîß How to Use

üëâ You can copy all YAMLs below into a file like `main.yaml` and run:

```bash
kubectl apply -f main.yaml
```

### üß™ 1Ô∏è‚É£ Pod example with All Three Probes

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: probe-demo
spec:
  containers:
  - name: demo-app
    image: httpd:latest
    ports:
    - containerPort: 8080
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
      initialDelaySeconds: 10
      periodSeconds: 5
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 3
    startupProbe:
      httpGet:
        path: /startup
        port: 8080
      failureThreshold: 30
      periodSeconds: 10
``` 

## ‚è±Ô∏è Probe Timing Example (Understanding Failure/Success)

Let's say you have this probe configuration:

```yaml
initialDelaySeconds: 15
periodSeconds: 30
failureThreshold: 4
successThreshold: 2
timeoutSeconds: 10
```

| Time       | Event               | Result            |
| ---------- | ------------------- | ----------------- |
| 2:00:00 PM | Pod Started         |                   |
| 2:00:15 PM | 1st Check (Fail)    | ‚ùå                 |
| 2:00:45 PM | 2nd Check (Fail)    | ‚ùå                 |
| 2:01:15 PM | 3rd Check (Fail)    | ‚ùå                 |
| 2:01:45 PM | 4th Check (Fail)    | ‚ùå Pod Restart     |
| 2:02:15 PM | 5th Check (Success) | ‚úÖ                 |
| 2:02:45 PM | 6th Check (Success) | ‚úÖ (Healthy again) |

> üí° Note: The probe starts only after the initialDelaySeconds. If failureThreshold is reached with consecutive failures, the container will be restarted. After restart, the container must pass successThreshold consecutive checks to be marked as Ready.

### üß™ 2Ô∏è‚É£ Liveness Probe with NGINX

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-nginx
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
    livenessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 10
      periodSeconds: 5
      failureThreshold: 3

```

### üß™ 3Ô∏è‚É£ Liveness Probe with Exec (BusyBox)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-busybox
spec:
  containers:
  - name: busybox
    image: busybox:latest
    command: ["sh", "-c", "touch /tmp/healthy && sleep 3600"]
    livenessProbe:
      exec:
        command: ["cat", "/tmp/healthy"]
      initialDelaySeconds: 5
      periodSeconds: 10
      failureThreshold: 3

```

### üèÅ 4Ô∏è‚É£ Init Container Example

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: init-demo
spec:
  containers:
  - name: app
    image: busybox
    command: ['sh', '-c', 'echo "App started!" && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'echo "Initializing..."; sleep 5']
```

### ü§ù 5Ô∏è‚É£ Sidecar Pattern ‚Äì Logging Example

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sidecar-demo
spec:
  containers:
  - name: app
    image: busybox
    command: ['sh', '-c', 'echo "Writing log..." && while true; do echo log >> /shared/log.txt; sleep 5; done']
    volumeMounts:
    - name: shared-logs
      mountPath: /shared

  - name: log-watcher
    image: busybox
    command: ['sh', '-c', 'tail -f /shared/log.txt']
    volumeMounts:
    - name: shared-logs
      mountPath: /shared

  volumes:
  - name: shared-logs
    emptyDir: {}
```

### üåø 6Ô∏è‚É£ Using Environment Variables  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: env-demo
spec:
  containers:
  - name: demo
    image: busybox
    command: ["sh", "-c", "echo $ENV_MSG && sleep 3600"]
    env:
    - name: ENV_MSG
      value: "Hello from LearnWithMithran Youtube!"
```

### ‚öôÔ∏è 7Ô∏è‚É£ ConfigMap and Secret as envFrom

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLR: blue
  APP_MODE: prod
---
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  Channel: TGVhcldpdGhNaXRocmFuCg==
  Owner: TWl0aHJhbgo=
---
apiVersion: v1
kind: Pod
metadata:
  name: simple-webcolor-pod
spec:
  containers:
  - name: simple-webcolor-pod
    image: httpd
    ports:
    - containerPort: 80
    envFrom:
    - configMapRef:
        name: app-config
    - secretRef:
        name: app-secret
```

### üîê 8Ô∏è‚É£ Secrets as Environment Variable and Mounted File

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: demo-secret
data:
  password: dXBkYXRlZDQ1Ngo=
---
apiVersion: v1
kind: Pod
metadata:
  name: secret-demo
spec:
  containers:
  - name: demo
    image: busybox
    command: ["/bin/sh", "-c", "while true; do echo ENV: $PASSWORD; echo FILE: $(cat /etc/secret/password); sleep 10; done"]
    env:
    - name: PASSWORD
      valueFrom:
        secretKeyRef:
          name: demo-secret
          key: password
    volumeMounts:
    - name: secret-vol
      mountPath: /etc/secret
      readOnly: true
  volumes:
  - name: secret-vol
    secret:
      secretName: demo-secret
```


# üöÄ Kubernetes Part 8 ‚Äì AWS EKS ALB Ingress Controller


## üìö What You Will Learn

In **Day 8**, you'll learn how to configure advanced Kubernetes networking and traffic routing using the **AWS ALB Ingress Controller** and **Helm**. This session helps you build real-world multi-team infrastructure in an Amazon EKS cluster.

‚úÖ Install and configure eksctl and Helm for production-grade clusters
‚úÖ Deploy the AWS Load Balancer Controller using Helm and IAM integration
‚úÖ Create and expose applications using Kubernetes Ingress
‚úÖ Set up path-based routing with a shared ALB across teams
‚úÖ Deploy workloads in isolated namespaces (e.g., team-a, team-b)
‚úÖ Use annotations to control Ingress behavior, grouping, and routing
‚úÖ Understand the difference between LoadBalancer and Ingress services
‚úÖ Deploy multi-container apps and manage traffic securely and efficiently

---

## ‚ùó Why Not Use LoadBalancer Services Alone?

Using Kubernetes Service of type LoadBalancer works well for exposing single services, but it has major limitations:

### ‚ö†Ô∏è Drawbacks of LoadBalancer Service

- üö´ One ALB per Service ‚Üí costly in production
- üìà Scales poorly if you have many microservices
- üß± No support for path-based or host-based routing
- üßç‚Äç‚ôÇÔ∏è Harder to manage for multiple teams/projects
- üîê Difficult to apply centralized security policies

## üß™ Example Issue with LoadBalancer Services

```yaml
apiVersion: v1
kind: Service
metadata:
  name: lb-service-1
spec:
  type: LoadBalancer
  selector:
    colour: blue
  ports:
    - port: 80
      targetPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  replicas: 5
  selector:
    matchLabels:
      colour: blue
  template:
    metadata:
      labels:
        colour: blue
    spec:
      containers:
      - name: cont1
        image: httpd
---
apiVersion: v1
kind: Service
metadata:
  name: lb-service-2
spec:
  type: LoadBalancer
  selector:
    colour: black
  ports:
    - port: 80
      targetPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-2
spec:
  replicas: 5
  selector:
    matchLabels:
      colour: black
  template:
    metadata:
      labels:
        colour: black
    spec:
      containers:
      - name: cont1
        image: nginx
```

## ‚úÖ Why Use Ingress with ALB Controller?

Ingress with ALB controller provides:

- ‚úÖ Single ALB for multiple apps (shared ALB)
- üîÅ Path-based routing (/app1, /app2, etc.)
- üßë‚Äçü§ù‚Äçüßë Team-level separation via namespaces & ingress group
- üí∏ Cost savings ‚Äì fewer ALBs created
- üîê Centralized control over routing & TLS termination
- ‚òÅÔ∏è Better AWS-native integration via Load Balancer Controller

> ‚úÖ Recommendation: Use Ingress + AWS Load Balancer Controller for scalable, secure, and cost-efficient EKS routing.


## ‚öôÔ∏è Prerequisites to Install ALB Ingress Controller

- AWS CLI configured
- IAM user with appropriate EKS permissions
- `kubectl` installed
- `eksctl` and `helm` installed

### üß± eksctl Setup

```bash
# Install eksctl
ARCH=amd64
PLATFORM=$(uname -s)_$ARCH

curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check

tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
sudo install -m 0755 /tmp/eksctl /usr/local/bin && rm /tmp/eksctl
```

### üîê OIDC and IAM Setup

```bash
eksctl utils associate-iam-oidc-provider \
  --region=ap-southeast-1 \
  --cluster=LearnWithMithran2 \
  --approve
```

### üì• Download the IAM policy:

[Click Me for IAM Json Policy](https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/refs/heads/main/docs/install/iam_policy.json)

Create the IAM policy in AWS as: `AWSLoadBalancerControllerIAMPolicy2`

### üì• Create service account:

```bash
eksctl create iamserviceaccount \
  --cluster LearnWithMithran2 \
  --region ap-southeast-1 \
  --namespace kube-system \
  --name aws-load-balancer-controller2 \
  --attach-policy-arn arn:aws:iam::992382429239:policy/AWSLoadBalancerControllerIAMPolicy2 \
  --approve
```

### üì¶ Helm Setup

```bash
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
```

### üì° Add and update repo:

```bash
helm repo add eks https://aws.github.io/eks-charts
helm repo update
```

### üöÄ Install AWS ALB Ingress Controller

```bash
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=LearnWithMithran \
  --set region=ap-southeast-1 \
  --set vpcId=vpc-020d3da5a9615c591 \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller2
```

---

## üåê Ingress Setup: Basic Echo App


```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hello
  template:
    metadata:
      labels:
        app: hello
    spec:
      containers:
      - name: hello
        image: hashicorp/http-echo
        args: ["-text=Hello from app"]
        ports:
        - containerPort: 5678
---
apiVersion: v1
kind: Service
metadata:
  name: hello-service
spec:
  selector:
    app: hello
  ports:
  - port: 80
    targetPort: 5678
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hello-ingress
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
spec:
  rules:
  - http:
      paths:
      - path: /LWM
        pathType: Prefix
        backend:
          service:
            name: hello-service
            port:
              number: 80
```

## üë• Multi-Team Namespaces with Shared ALB

### Create Namespaces

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: team-a
---
apiVersion: v1
kind: Namespace
metadata:
  name: team-b
```

### Team A

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-a
  namespace: team-a
spec:
  replicas: 3
  selector:
    matchLabels:
      app: app-a
  template:
    metadata:
      labels:
        app: app-a
    spec:
      containers:
      - name: app-a
        image: hashicorp/http-echo
        args: ["-text=Hello from App A LWM"]
        ports:
        - containerPort: 5678
---
apiVersion: v1
kind: Service
metadata:
  name: app-a-svc
  namespace: team-a
spec:
  selector:
    app: app-a
  ports:
  - port: 80
    targetPort: 5678
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-a-ingress
  namespace: team-a
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/group.name: shared-alb
spec:
  rules:
  - http:
      paths:
      - path: /a
        pathType: Prefix
        backend:
          service:
            name: app-a-svc
            port:
              number: 80
```

### Team B
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-b
  namespace: team-b
spec:
  replicas: 3
  selector:
    matchLabels:
      app: app-b
  template:
    metadata:
      labels:
        app: app-b
    spec:
      containers:
      - name: app-b
        image: hashicorp/http-echo
        args: ["-text=Hello from App B LWM"]
        ports:
        - containerPort: 5678
---
apiVersion: v1
kind: Service
metadata:
  name: app-b-svc
  namespace: team-b
spec:
  selector:
    app: app-b
  ports:
  - port: 80
    targetPort: 5678
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-b-ingress
  namespace: team-b
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/group.name: shared-alb
spec:
  rules:
  - http:
      paths:
      - path: /b
        pathType: Prefix
        backend:
          service:
            name: app-b-svc
            port:
              number: 80
```

## üìå Notes

- Ingress class should match your ALB controller: kubernetes.io/ingress.class: alb
- Shared ALB group enables multi-team routing via paths
- Ensure VPC ID and IAM policies are correctly set in the helm install step

---

# üöÄ Kubernetes Part 9 ‚Äì Jobs, CronJobs, StatefulSets, HPA, and ExternalName Services


## üìö What You Will Learn

In **Day 9**, we deep dive into *Kubernetes workload types and resource management*. These examples are hands-on and ideal for mastering real-world use cases with **Jobs, CronJobs, StatefulSets, ExternalName, Resource Requests/Limits, and Horizontal Pod Autoscaling (HPA)**.

‚úÖ Create one-time Jobs and scheduled CronJobs
‚úÖ Understand StatefulSet and Headless Services (with and without volumes)
‚úÖ Simulate FQDN with ExternalName services
‚úÖ Apply resource requests and limits to containers
‚úÖ Configure HPA using the Kubernetes Metrics Server
‚úÖ Deploy multi-replica applications using Deployments and Services
‚úÖ Run test pods and simulate DNS-based access

---

## üõ†Ô∏è Kubernetes Job and CronJob ‚Äì One-time and Recurring Tasks

Jobs and CronJobs help run short-lived or recurring background tasks in Kubernetes.

### üî® What is a Job?
A Job creates one or more pods and ensures that a specific task runs to completion. Once the task finishes successfully, the Job is considered complete.

‚úÖ Use when you need to:

- Run a one-time task
- Perform data migration or setup scripts
- Process a batch job or backup operation

#### üîß Example YAML ‚Äì Job

```yaml
# One-time Job
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-job
spec:
  template:
    spec:
      containers:
      - name: hello
        image: busybox
        command: ["echo", "Hello from Kubernetes Job"]
      restartPolicy: Never
  backoffLimit: 2
```


### ‚è∞ What is a CronJob?
A CronJob creates Jobs on a schedule, like a traditional Linux cron. It‚Äôs ideal for repeating tasks such as log rotation, cleanup, backups, etc.

‚úÖ Use when you need to:

- Run a task at specific time intervals
- Automate recurring operations
- Perform periodic cleanups or reports

#### üîß Example YAML ‚Äì CronJob

```yaml
# Recurring CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cleanup-task
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cleanup
            image: busybox
            command: ["sh", "-c", "echo Cleaning temp files at $(date)"]
          restartPolicy: OnFailure
```

### üÜö Jobs vs CronJobs ‚Äì Comparison Table

| Feature            | **Job**                                     | **CronJob**                       |
| ------------------ | ------------------------------------------- | --------------------------------- |
| **Run Type**       | One-time execution                          | Recurring on schedule             |
| **Scheduling**     | Immediate or manual                         | CRON-based (e.g., every 5 mins)   |
| **Use Case**       | Backup, data migration, processing one file | Daily reports, cleanup tasks      |
| **Pod Lifecycle**  | Runs once then terminates                   | Triggers Job objects at intervals |
| **Built-in Retry** | ‚úÖ Yes (`backoffLimit`)                      | ‚úÖ Yes, inherited from Job spec    |


---

## ‚öîÔ∏è Deployments vs StatefulSets

Kubernetes provides different workload types for managing application lifecycles. Two of the most common are Deployments and StatefulSets.


### ‚úÖ When to Use Deployments

- Your application is stateless
- You want easy horizontal scaling
- Any pod can serve any request (e.g., web servers, REST APIs, frontend apps)
- Persistent storage is not required or shared storage is enough

### ‚úÖ When to Use StatefulSets

- Each pod needs a stable identity
- You want stable persistent storage per pod
- Pods need stable DNS names
- You require ordered, graceful deployment, scaling, or deletion
- Use cases include databases (MySQL, MongoDB), queues, Kafka, Zookeeper

### üîç What‚Äôs the Difference?

| Feature                    | **Deployment**                                              | **StatefulSet**                                                    |
| -------------------------- | ----------------------------------------------------------- | ------------------------------------------------------------------ |
| **Pod Name**               | Dynamic, auto-generated (e.g., `app-xyz123`)                | Stable and predictable (`app-0`, `app-1`)                          |
| **Pod Identity**           | All pods are identical (no unique identity)                 | Each pod has a **unique, stable identity**                         |
| **Storage**                | Shared or ephemeral storage                                 | Dedicated persistent volume per pod                                |
| **Network Identity (DNS)** | Shared service FQDN (e.g., `svc.default.svc.cluster.local`) | Unique FQDN per pod (e.g., `pod-0.svc.default.svc.cluster.local`)  |
| **Scaling Behavior**       | All pods are interchangeable                                | Pods are created **sequentially** and deleted in **reverse order** |
| **Use Case**               | Stateless applications                                      | Stateful applications                                              |


## üß± StatefulSet With Headless Service (Without Volumes)

```yaml
# Headless Service
apiVersion: v1
kind: Service
metadata:
  name: nginx-headless
spec:
  clusterIP: None
  selector:
    app: nginx
  ports:
  - port: 80
    name: http
---
# StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx
spec:
  serviceName: "nginx-headless"
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```

## üß± Deployment With ClusterIp Service

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-deploy
spec:
  replicas: 5
  selector:
    matchLabels:
      colour: blue
  template:
    metadata:
      labels:
        colour: blue
    spec:
      containers:
      - name: cont1
        image: httpd
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: httpd-svc
spec:
  selector:
    colour: blue
  ports:
  - port: 80
    targetPort: 80
```

## üîÑ StatefulSet vs Deployment ‚Äì FQDN Behavior and Identity

When running microservices in Kubernetes, how services are addressed (FQDN) matters for stable communication, especially in distributed systems like databases or stateful workloads.

| Feature                 | **Deployment**                                 | **StatefulSet**                                                  |
| ----------------------- | ---------------------------------------------- | ---------------------------------------------------------------- |
| **Pod Name**            | Random (e.g., `httpd-deploy-5f89dfd9cc-abcde`) | Fixed (e.g., `nginx-0`, `nginx-1`, `nginx-2`)                    |
| **ClusterIP**           | Assigned (e.g., `10.100.247.159`)              | `None` (Headless Service)                                        |
| **Pod IP**              | Dynamic (changes on restart)                   | Dynamic (still can change, but FQDN is stable)                   |
| **FQDN Format**         | `httpd-svc.default.svc.cluster.local`          | `nginx-0.nginx-headless.default.svc.cluster.local`, `nginx-1...` |
| **Stable DNS Identity** | ‚ùå No stable per-pod DNS                        | ‚úÖ Stable and addressable via predictable FQDN                    |
| **Use Case**            | Stateless apps (e.g., web servers)             | Stateful apps (e.g., DBs, queues)                                |

## üìò Example: FQDN Usage

```
# Deployment
FQDN ‚Üí httpd-svc.default.svc.cluster.local
Pod IP ‚Üí Changes on restart (e.g., 172.31.4.118)

# StatefulSet
FQDNs ‚Üí
- nginx-0.nginx-headless.default.svc.cluster.local
- nginx-1.nginx-headless.default.svc.cluster.local
- nginx-2.nginx-headless.default.svc.cluster.local

Pod IP ‚Üí May change, but name-based resolution is stable
```

## üß™ Testing DNS FQDN from a dummy Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: dns-test
spec:
  containers:
  - name: test
    image: curlimages/curl
    command: ["sleep", "3600"]
```

## üì¶ StatefulSet With Persistent Volumes ==(need PV & PVC)==

```yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-headless
spec:
  clusterIP: None
  selector:
    app: redis
  ports:
  - port: 6379
    name: redis
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
spec:
  serviceName: "redis-headless"
  replicas: 3
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis
        ports:
        - containerPort: 6379
        volumeMounts:
        - name: redis-data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: redis-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi

```

## üåê Kubernetes ExternalName Service ‚Äì Access External Resources via DNS

### üîç What is an ExternalName Service?
An ExternalName service is a special type of Kubernetes Service that allows pods inside the cluster to access an external service (outside the cluster) using an internal DNS name.

Instead of routing traffic to internal pods or endpoints, it returns a CNAME (DNS alias) pointing to an external FQDN.

### üîß Example Use Cases

‚úÖ Allow applications to access:
- An external database (e.g., RDS, MongoDB Atlas)
- A legacy app running outside Kubernetes (e.g., on EC2)
- Third-party APIs or SaaS systems (e.g., Google, Stripe)

#### üõ†Ô∏è Sample YAML ‚Äì ExternalName to EC2 Endpoint

```yaml
# EC2-hosted app
apiVersion: v1
kind: Service
metadata:
  name: svc
spec:
  type: ExternalName
  externalName: ec2-52-77-254-161.ap-southeast-1.compute.amazonaws.com
```

#### üîß Sample YAML ‚Äì ExternalName to Public Website (Google)

```yaml
# Google ExternalName (for test/demo only)
apiVersion: v1
kind: Service
metadata:
  name: google-svc
spec:
  type: ExternalName
  externalName: www.google.com
```

---

## ‚öôÔ∏è Kubernetes Resource Requests and Limits ‚Äì Manage Pod Resource Usage

### üîç What Are Requests and Limits?
Kubernetes allows you to control how much CPU and memory a container can request and use through requests and limits. This helps ensure fair resource allocation, avoid overconsumption, and enable auto-scaling.

### üß† Key Concepts

| Term           | Meaning                                                                |
| -------------- | ---------------------------------------------------------------------- |
| **Request**    | The **minimum** amount of CPU/memory guaranteed to the container       |
| **Limit**      | The **maximum** amount of CPU/memory the container is allowed to use   |
| **Eviction**   | If a pod exceeds limits or system is under pressure, it may be evicted |
| **Throttling** | CPU usage above limit is throttled (not evicted)                       |

#### üîß Example YAML ‚Äì Resource Request and Limit

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-rl
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "200m"
        memory: "256Mi"
```

#### üí° CPU Units Explained

| Value  | Meaning         |
| ------ | --------------- |
| `1`    | 1 full CPU core |
| `500m` | 0.5 CPU core    |
| `100m` | 0.1 CPU core    |


#### üí° Memory Units
Common formats: Mi, Gi, M, G

Example: 128Mi = 128 Mebibytes

### ‚öôÔ∏è What Happens at Runtime?
- If the node runs out of resources:
  - Pods may be evicted if they exceed memory limits
  - CPU usage will be throttled if it exceeds CPU limits
- Requests help the scheduler decide where to place pods

### üß™ Real-World Use Case

| Scenario                   | Request              | Limit                |
| -------------------------- | -------------------- | -------------------- |
| Small API                  | 100m CPU / 128Mi RAM | 200m CPU / 256Mi RAM |
| Heavy Worker               | 500m CPU / 512Mi RAM | 1 CPU / 1Gi RAM      |
| Memory-intensive Batch Job | 256Mi RAM            | 1Gi RAM              |

---

## üìà HPA vs VPA ‚Äì Kubernetes Auto Scaling Demystified
In Kubernetes, auto-scaling ensures that your application has the right number of resources and replicas based on real-time demand. The two main types of autoscalers are:

- **Horizontal Pod Autoscaler (HPA)** ‚Äì scales the number of pod replicas
- **Vertical Pod Autoscaler (VPA)** ‚Äì adjusts the resource requests/limits for each pod

### üîÑ What is HPA (Horizontal Pod Autoscaler)?
HPA automatically adjusts the number of pods in a Deployment, ReplicaSet, or StatefulSet based on observed CPU/memory usage or custom metrics.

### ‚úÖ Use when:

- You want to scale out/in based on traffic/load
- Your app is stateless
- Metrics are available via Metrics Server

#### üîß Example YAML ‚Äì HPA

```yaml
# Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  selector:
    matchLabels:
      run: php-apache
  template:
    metadata:
      labels:
        run: php-apache
    spec:
      containers:
      - name: php-apache
        image: registry.k8s.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m
---
# Service
apiVersion: v1
kind: Service
metadata:
  name: php-apache
spec:
  ports:
  - port: 80
  selector:
    run: php-apache
---
# HPA (Metrics Server Required)
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
```

### üß± What is VPA (Vertical Pod Autoscaler)?
VPA automatically adjusts the CPU and memory requests/limits of containers in a pod to better match the actual usage.

### ‚úÖ Use when:

- You want to optimize resource usage per pod
- Your app does not scale well horizontally
- You need to reduce over-provisioning

#### üîß Modes in VPA:

| Mode      | Behavior                         |
| --------- | -------------------------------- |
| `Off`     | Just monitor; no action          |
| `Auto`    | Actively updates pod resources   |
| `Initial` | Only sets values on pod creation |

#### Example YAML ‚Äì VPA

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: php-apache-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind:       Deployment
    name:       php-apache
  updatePolicy:
    updateMode: "Auto"
```
